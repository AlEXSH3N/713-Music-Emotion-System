{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from keras_tuner import RandomSearch, HyperParameters, Objective\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解压 cleaned_lyrics.zip 文件\n",
    "with zipfile.ZipFile('cleaned_lyrics.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('cleaned_lyrics')\n",
    "\n",
    "# 获取所有歌词文件的路径\n",
    "lyrics_files = {os.path.splitext(f)[0]: os.path.join('cleaned_lyrics', f) for f in os.listdir('cleaned_lyrics')}\n",
    "\n",
    "# 读取 filtered_dataset.csv 文件\n",
    "data = pd.read_csv('filtered_dataset.csv')\n",
    "\n",
    "def read_lyrics(record_id):\n",
    "    file_path = lyrics_files.get(str(record_id))\n",
    "    if file_path and os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    return ''\n",
    "\n",
    "# 读取歌词并添加到数据框中\n",
    "data['lyrics'] = data['record_id'].apply(read_lyrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>record_id</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>...</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "      <th>valence_bin</th>\n",
       "      <th>energy_bin</th>\n",
       "      <th>danceability_bin</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>740</td>\n",
       "      <td>1T7Tqsfkz0Ntbwta2hHebY</td>\n",
       "      <td>Days N Daze</td>\n",
       "      <td>Rogue Taxidermy</td>\n",
       "      <td>Fate of a Coward</td>\n",
       "      <td>28</td>\n",
       "      <td>175046</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3510</td>\n",
       "      <td>0.9430</td>\n",
       "      <td>123.094</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>mind seeping darkness pulse growing weaker mom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>513</td>\n",
       "      <td>27hFQQS3cVUmIK3ser5bpu</td>\n",
       "      <td>Colin &amp; Caroline</td>\n",
       "      <td>More Than Gravity</td>\n",
       "      <td>More Than Gravity</td>\n",
       "      <td>34</td>\n",
       "      <td>266078</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.2170</td>\n",
       "      <td>110.969</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>simple explanation things feel one word tell t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>136</td>\n",
       "      <td>2PIlBukQ6limukVR8Ubb5o</td>\n",
       "      <td>Gabrielle Aplin</td>\n",
       "      <td>English Rain</td>\n",
       "      <td>Please Don't Say You Love Me</td>\n",
       "      <td>59</td>\n",
       "      <td>181400</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>85.994</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>summer comes winter fades not pressure not cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>899</td>\n",
       "      <td>4uPvXmXGjYOOqhbRMmS9XU</td>\n",
       "      <td>Grace Petrie</td>\n",
       "      <td>Queer As Folk</td>\n",
       "      <td>Northbound</td>\n",
       "      <td>26</td>\n",
       "      <td>270920</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.5860</td>\n",
       "      <td>125.642</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>gone lonesome road goes forever espresso shot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>549</td>\n",
       "      <td>2saK0E712wIB3Gf8QLuFYX</td>\n",
       "      <td>Get Dead</td>\n",
       "      <td>Dancing with the Curse</td>\n",
       "      <td>Nickel Plated</td>\n",
       "      <td>29</td>\n",
       "      <td>136815</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6540</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>179.920</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>nickel plated tooth briefcase hold breath poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5896</th>\n",
       "      <td>10802</td>\n",
       "      <td>10802</td>\n",
       "      <td>113500</td>\n",
       "      <td>2kDe0QRaCBKIh8QY64GDvK</td>\n",
       "      <td>Bethel Music;Jenn Johnson</td>\n",
       "      <td>We Will Not Be Shaken (Live)</td>\n",
       "      <td>In Over My Head (Crash Over Me) - Live</td>\n",
       "      <td>49</td>\n",
       "      <td>298681</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>141.800</td>\n",
       "      <td>3</td>\n",
       "      <td>world-music</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>come place life full satisfied longing feel he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5897</th>\n",
       "      <td>10803</td>\n",
       "      <td>10803</td>\n",
       "      <td>113774</td>\n",
       "      <td>4EQYur0tRZpHbQJxgCRy4Q</td>\n",
       "      <td>Michael W. Smith</td>\n",
       "      <td>Worship</td>\n",
       "      <td>More Love, More Power - Live</td>\n",
       "      <td>39</td>\n",
       "      <td>310293</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.3440</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>142.075</td>\n",
       "      <td>4</td>\n",
       "      <td>world-music</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>love power life love power life worship heart ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5898</th>\n",
       "      <td>10805</td>\n",
       "      <td>10805</td>\n",
       "      <td>113108</td>\n",
       "      <td>6CW9qtzZpHZ3o39BYlpU0x</td>\n",
       "      <td>Bethel Music;Bethany Wohrle</td>\n",
       "      <td>Living Hope</td>\n",
       "      <td>Living Hope</td>\n",
       "      <td>53</td>\n",
       "      <td>406346</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5290</td>\n",
       "      <td>0.1850</td>\n",
       "      <td>143.957</td>\n",
       "      <td>4</td>\n",
       "      <td>world-music</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>great chasm lay high mountain climb desperatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5899</th>\n",
       "      <td>10807</td>\n",
       "      <td>10807</td>\n",
       "      <td>113056</td>\n",
       "      <td>2elEVvWjPZltkotzcCwKvM</td>\n",
       "      <td>Kari Jobe;Cody Carnes;Elevation Worship</td>\n",
       "      <td>The Blessing (Live)</td>\n",
       "      <td>The Blessing - Live</td>\n",
       "      <td>61</td>\n",
       "      <td>514665</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>140.015</td>\n",
       "      <td>4</td>\n",
       "      <td>world-music</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>lord bless keep make face shine upon gracious ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>10808</td>\n",
       "      <td>10808</td>\n",
       "      <td>113757</td>\n",
       "      <td>2q7NWcqIPqAr6bBF6Heqs7</td>\n",
       "      <td>Hillsong Kids</td>\n",
       "      <td>Can You Believe It!?</td>\n",
       "      <td>Who You Say I Am</td>\n",
       "      <td>36</td>\n",
       "      <td>239040</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0868</td>\n",
       "      <td>0.3240</td>\n",
       "      <td>171.987</td>\n",
       "      <td>3</td>\n",
       "      <td>world-music</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>highest king welcome lost brought oh love oh l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5901 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.1  record_id  Unnamed: 0                track_id   \n",
       "0                1          1         740  1T7Tqsfkz0Ntbwta2hHebY  \\\n",
       "1                3          3         513  27hFQQS3cVUmIK3ser5bpu   \n",
       "2                4          4         136  2PIlBukQ6limukVR8Ubb5o   \n",
       "3                5          5         899  4uPvXmXGjYOOqhbRMmS9XU   \n",
       "4                9          9         549  2saK0E712wIB3Gf8QLuFYX   \n",
       "...            ...        ...         ...                     ...   \n",
       "5896         10802      10802      113500  2kDe0QRaCBKIh8QY64GDvK   \n",
       "5897         10803      10803      113774  4EQYur0tRZpHbQJxgCRy4Q   \n",
       "5898         10805      10805      113108  6CW9qtzZpHZ3o39BYlpU0x   \n",
       "5899         10807      10807      113056  2elEVvWjPZltkotzcCwKvM   \n",
       "5900         10808      10808      113757  2q7NWcqIPqAr6bBF6Heqs7   \n",
       "\n",
       "                                      artists                    album_name   \n",
       "0                                 Days N Daze               Rogue Taxidermy  \\\n",
       "1                            Colin & Caroline             More Than Gravity   \n",
       "2                             Gabrielle Aplin                  English Rain   \n",
       "3                                Grace Petrie                 Queer As Folk   \n",
       "4                                    Get Dead        Dancing with the Curse   \n",
       "...                                       ...                           ...   \n",
       "5896                Bethel Music;Jenn Johnson  We Will Not Be Shaken (Live)   \n",
       "5897                         Michael W. Smith                       Worship   \n",
       "5898              Bethel Music;Bethany Wohrle                   Living Hope   \n",
       "5899  Kari Jobe;Cody Carnes;Elevation Worship           The Blessing (Live)   \n",
       "5900                            Hillsong Kids          Can You Believe It!?   \n",
       "\n",
       "                                  track_name  popularity  duration_ms   \n",
       "0                           Fate of a Coward          28       175046  \\\n",
       "1                          More Than Gravity          34       266078   \n",
       "2               Please Don't Say You Love Me          59       181400   \n",
       "3                                 Northbound          26       270920   \n",
       "4                              Nickel Plated          29       136815   \n",
       "...                                      ...         ...          ...   \n",
       "5896  In Over My Head (Crash Over Me) - Live          49       298681   \n",
       "5897            More Love, More Power - Live          39       310293   \n",
       "5898                             Living Hope          53       406346   \n",
       "5899                     The Blessing - Live          61       514665   \n",
       "5900                        Who You Say I Am          36       239040   \n",
       "\n",
       "      explicit  ...  instrumentalness  liveness  valence    tempo   \n",
       "0        False  ...          0.000000    0.3510   0.9430  123.094  \\\n",
       "1        False  ...          0.000039    0.1060   0.2170  110.969   \n",
       "2        False  ...          0.000000    0.1080   0.3210   85.994   \n",
       "3        False  ...          0.000000    0.1410   0.5860  125.642   \n",
       "4         True  ...          0.000000    0.6540   0.4000  179.920   \n",
       "...        ...  ...               ...       ...      ...      ...   \n",
       "5896     False  ...          0.029400    0.0833   0.0896  141.800   \n",
       "5897     False  ...          0.000094    0.3440   0.1510  142.075   \n",
       "5898     False  ...          0.000000    0.5290   0.1850  143.957   \n",
       "5899     False  ...          0.000000    0.2220   0.1970  140.015   \n",
       "5900     False  ...          0.000000    0.0868   0.3240  171.987   \n",
       "\n",
       "      time_signature  track_genre  valence_bin  energy_bin  danceability_bin   \n",
       "0                  4     acoustic            2         1.0                 1  \\\n",
       "1                  4     acoustic            0         1.0                 1   \n",
       "2                  4     acoustic            0         1.0                 1   \n",
       "3                  4     acoustic            1         2.0                 1   \n",
       "4                  4     acoustic            1         2.0                 1   \n",
       "...              ...          ...          ...         ...               ...   \n",
       "5896               3  world-music            0         1.0                 1   \n",
       "5897               4  world-music            0         0.0                 0   \n",
       "5898               4  world-music            0         2.0                 1   \n",
       "5899               4  world-music            0         1.0                 1   \n",
       "5900               3  world-music            0         1.0                 1   \n",
       "\n",
       "                                                 lyrics  \n",
       "0     mind seeping darkness pulse growing weaker mom...  \n",
       "1     simple explanation things feel one word tell t...  \n",
       "2     summer comes winter fades not pressure not cha...  \n",
       "3     gone lonesome road goes forever espresso shot ...  \n",
       "4     nickel plated tooth briefcase hold breath poli...  \n",
       "...                                                 ...  \n",
       "5896  come place life full satisfied longing feel he...  \n",
       "5897  love power life love power life worship heart ...  \n",
       "5898  great chasm lay high mountain climb desperatio...  \n",
       "5899  lord bless keep make face shine upon gracious ...  \n",
       "5900  highest king welcome lost brought oh love oh l...  \n",
       "\n",
       "[5901 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5901"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lyrics_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>record_id</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>...</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "      <th>valence_bin</th>\n",
       "      <th>energy_bin</th>\n",
       "      <th>danceability_bin</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>740</td>\n",
       "      <td>1T7Tqsfkz0Ntbwta2hHebY</td>\n",
       "      <td>Days N Daze</td>\n",
       "      <td>Rogue Taxidermy</td>\n",
       "      <td>Fate of a Coward</td>\n",
       "      <td>28</td>\n",
       "      <td>175046</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.3510</td>\n",
       "      <td>0.9430</td>\n",
       "      <td>123.094</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>mind seeping darkness pulse growing weaker mom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>513</td>\n",
       "      <td>27hFQQS3cVUmIK3ser5bpu</td>\n",
       "      <td>Colin &amp; Caroline</td>\n",
       "      <td>More Than Gravity</td>\n",
       "      <td>More Than Gravity</td>\n",
       "      <td>34</td>\n",
       "      <td>266078</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.1060</td>\n",
       "      <td>0.2170</td>\n",
       "      <td>110.969</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>simple explanation things feel one word tell t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>136</td>\n",
       "      <td>2PIlBukQ6limukVR8Ubb5o</td>\n",
       "      <td>Gabrielle Aplin</td>\n",
       "      <td>English Rain</td>\n",
       "      <td>Please Don't Say You Love Me</td>\n",
       "      <td>59</td>\n",
       "      <td>181400</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.3210</td>\n",
       "      <td>85.994</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>summer comes winter fades not pressure not cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>899</td>\n",
       "      <td>4uPvXmXGjYOOqhbRMmS9XU</td>\n",
       "      <td>Grace Petrie</td>\n",
       "      <td>Queer As Folk</td>\n",
       "      <td>Northbound</td>\n",
       "      <td>26</td>\n",
       "      <td>270920</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.5860</td>\n",
       "      <td>125.642</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>gone lonesome road goes forever espresso shot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>549</td>\n",
       "      <td>2saK0E712wIB3Gf8QLuFYX</td>\n",
       "      <td>Get Dead</td>\n",
       "      <td>Dancing with the Curse</td>\n",
       "      <td>Nickel Plated</td>\n",
       "      <td>29</td>\n",
       "      <td>136815</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.6540</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>179.920</td>\n",
       "      <td>4</td>\n",
       "      <td>acoustic</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>nickel plated tooth briefcase hold breath poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5896</th>\n",
       "      <td>10802</td>\n",
       "      <td>10802</td>\n",
       "      <td>113500</td>\n",
       "      <td>2kDe0QRaCBKIh8QY64GDvK</td>\n",
       "      <td>Bethel Music;Jenn Johnson</td>\n",
       "      <td>We Will Not Be Shaken (Live)</td>\n",
       "      <td>In Over My Head (Crash Over Me) - Live</td>\n",
       "      <td>49</td>\n",
       "      <td>298681</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.0833</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>141.800</td>\n",
       "      <td>3</td>\n",
       "      <td>world-music</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>come place life full satisfied longing feel he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5897</th>\n",
       "      <td>10803</td>\n",
       "      <td>10803</td>\n",
       "      <td>113774</td>\n",
       "      <td>4EQYur0tRZpHbQJxgCRy4Q</td>\n",
       "      <td>Michael W. Smith</td>\n",
       "      <td>Worship</td>\n",
       "      <td>More Love, More Power - Live</td>\n",
       "      <td>39</td>\n",
       "      <td>310293</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.3440</td>\n",
       "      <td>0.1510</td>\n",
       "      <td>142.075</td>\n",
       "      <td>4</td>\n",
       "      <td>world-music</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>love power life love power life worship heart ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5898</th>\n",
       "      <td>10805</td>\n",
       "      <td>10805</td>\n",
       "      <td>113108</td>\n",
       "      <td>6CW9qtzZpHZ3o39BYlpU0x</td>\n",
       "      <td>Bethel Music;Bethany Wohrle</td>\n",
       "      <td>Living Hope</td>\n",
       "      <td>Living Hope</td>\n",
       "      <td>53</td>\n",
       "      <td>406346</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5290</td>\n",
       "      <td>0.1850</td>\n",
       "      <td>143.957</td>\n",
       "      <td>4</td>\n",
       "      <td>world-music</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>great chasm lay high mountain climb desperatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5899</th>\n",
       "      <td>10807</td>\n",
       "      <td>10807</td>\n",
       "      <td>113056</td>\n",
       "      <td>2elEVvWjPZltkotzcCwKvM</td>\n",
       "      <td>Kari Jobe;Cody Carnes;Elevation Worship</td>\n",
       "      <td>The Blessing (Live)</td>\n",
       "      <td>The Blessing - Live</td>\n",
       "      <td>61</td>\n",
       "      <td>514665</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.1970</td>\n",
       "      <td>140.015</td>\n",
       "      <td>4</td>\n",
       "      <td>world-music</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>lord bless keep make face shine upon gracious ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5900</th>\n",
       "      <td>10808</td>\n",
       "      <td>10808</td>\n",
       "      <td>113757</td>\n",
       "      <td>2q7NWcqIPqAr6bBF6Heqs7</td>\n",
       "      <td>Hillsong Kids</td>\n",
       "      <td>Can You Believe It!?</td>\n",
       "      <td>Who You Say I Am</td>\n",
       "      <td>36</td>\n",
       "      <td>239040</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0868</td>\n",
       "      <td>0.3240</td>\n",
       "      <td>171.987</td>\n",
       "      <td>3</td>\n",
       "      <td>world-music</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>highest king welcome lost brought oh love oh l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5901 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0.1  record_id  Unnamed: 0                track_id   \n",
       "0                1          1         740  1T7Tqsfkz0Ntbwta2hHebY  \\\n",
       "1                3          3         513  27hFQQS3cVUmIK3ser5bpu   \n",
       "2                4          4         136  2PIlBukQ6limukVR8Ubb5o   \n",
       "3                5          5         899  4uPvXmXGjYOOqhbRMmS9XU   \n",
       "4                9          9         549  2saK0E712wIB3Gf8QLuFYX   \n",
       "...            ...        ...         ...                     ...   \n",
       "5896         10802      10802      113500  2kDe0QRaCBKIh8QY64GDvK   \n",
       "5897         10803      10803      113774  4EQYur0tRZpHbQJxgCRy4Q   \n",
       "5898         10805      10805      113108  6CW9qtzZpHZ3o39BYlpU0x   \n",
       "5899         10807      10807      113056  2elEVvWjPZltkotzcCwKvM   \n",
       "5900         10808      10808      113757  2q7NWcqIPqAr6bBF6Heqs7   \n",
       "\n",
       "                                      artists                    album_name   \n",
       "0                                 Days N Daze               Rogue Taxidermy  \\\n",
       "1                            Colin & Caroline             More Than Gravity   \n",
       "2                             Gabrielle Aplin                  English Rain   \n",
       "3                                Grace Petrie                 Queer As Folk   \n",
       "4                                    Get Dead        Dancing with the Curse   \n",
       "...                                       ...                           ...   \n",
       "5896                Bethel Music;Jenn Johnson  We Will Not Be Shaken (Live)   \n",
       "5897                         Michael W. Smith                       Worship   \n",
       "5898              Bethel Music;Bethany Wohrle                   Living Hope   \n",
       "5899  Kari Jobe;Cody Carnes;Elevation Worship           The Blessing (Live)   \n",
       "5900                            Hillsong Kids          Can You Believe It!?   \n",
       "\n",
       "                                  track_name  popularity  duration_ms   \n",
       "0                           Fate of a Coward          28       175046  \\\n",
       "1                          More Than Gravity          34       266078   \n",
       "2               Please Don't Say You Love Me          59       181400   \n",
       "3                                 Northbound          26       270920   \n",
       "4                              Nickel Plated          29       136815   \n",
       "...                                      ...         ...          ...   \n",
       "5896  In Over My Head (Crash Over Me) - Live          49       298681   \n",
       "5897            More Love, More Power - Live          39       310293   \n",
       "5898                             Living Hope          53       406346   \n",
       "5899                     The Blessing - Live          61       514665   \n",
       "5900                        Who You Say I Am          36       239040   \n",
       "\n",
       "      explicit  ...  instrumentalness  liveness  valence    tempo   \n",
       "0        False  ...          0.000000    0.3510   0.9430  123.094  \\\n",
       "1        False  ...          0.000039    0.1060   0.2170  110.969   \n",
       "2        False  ...          0.000000    0.1080   0.3210   85.994   \n",
       "3        False  ...          0.000000    0.1410   0.5860  125.642   \n",
       "4         True  ...          0.000000    0.6540   0.4000  179.920   \n",
       "...        ...  ...               ...       ...      ...      ...   \n",
       "5896     False  ...          0.029400    0.0833   0.0896  141.800   \n",
       "5897     False  ...          0.000094    0.3440   0.1510  142.075   \n",
       "5898     False  ...          0.000000    0.5290   0.1850  143.957   \n",
       "5899     False  ...          0.000000    0.2220   0.1970  140.015   \n",
       "5900     False  ...          0.000000    0.0868   0.3240  171.987   \n",
       "\n",
       "      time_signature  track_genre  valence_bin  energy_bin  danceability_bin   \n",
       "0                  4     acoustic            2         1.0                 1  \\\n",
       "1                  4     acoustic            0         1.0                 1   \n",
       "2                  4     acoustic            0         1.0                 1   \n",
       "3                  4     acoustic            1         2.0                 1   \n",
       "4                  4     acoustic            1         2.0                 1   \n",
       "...              ...          ...          ...         ...               ...   \n",
       "5896               3  world-music            0         1.0                 1   \n",
       "5897               4  world-music            0         0.0                 0   \n",
       "5898               4  world-music            0         2.0                 1   \n",
       "5899               4  world-music            0         1.0                 1   \n",
       "5900               3  world-music            0         1.0                 1   \n",
       "\n",
       "                                                 lyrics  \n",
       "0     mind seeping darkness pulse growing weaker mom...  \n",
       "1     simple explanation things feel one word tell t...  \n",
       "2     summer comes winter fades not pressure not cha...  \n",
       "3     gone lonesome road goes forever espresso shot ...  \n",
       "4     nickel plated tooth briefcase hold breath poli...  \n",
       "...                                                 ...  \n",
       "5896  come place life full satisfied longing feel he...  \n",
       "5897  love power life love power life worship heart ...  \n",
       "5898  great chasm lay high mountain climb desperatio...  \n",
       "5899  lord bless keep make face shine upon gracious ...  \n",
       "5900  highest king welcome lost brought oh love oh l...  \n",
       "\n",
       "[5901 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/keras/src/utils/np_utils.py:62: RuntimeWarning: invalid value encountered in cast\n",
      "  y = np.array(y, dtype=\"int\")\n"
     ]
    }
   ],
   "source": [
    "# 使用 Tokenizer 处理文本\n",
    "max_words = 5000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['lyrics'])\n",
    "sequences = tokenizer.texts_to_sequences(data['lyrics'])\n",
    "X_lyrics = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# 准备标签\n",
    "y_valence = to_categorical(data['valence_bin'].values)\n",
    "y_energy = to_categorical(data['energy_bin'].values)\n",
    "y_danceability = to_categorical(data['danceability_bin'].values)\n",
    "\n",
    "# 拆分数据集\n",
    "X_train_val, X_test, y_train_val_valence, y_test_valence, y_train_val_energy, y_test_energy, y_train_val_danceability, y_test_danceability = train_test_split(\n",
    "    X_lyrics, y_valence, y_energy, y_danceability, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train_valence, y_val_valence, y_train_energy, y_val_energy, y_train_danceability, y_val_danceability = train_test_split(\n",
    "    X_train_val, y_train_val_valence, y_train_val_energy, y_train_val_danceability, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 229,  582,  229, ...,  146,  119,   31],\n",
       "       [   1,    7,  903, ...,   11,  142,   31],\n",
       "       [ 729,    1,  342, ...,   87,  694,   31],\n",
       "       ...,\n",
       "       [  39,    3,   16, ...,   45,  150,   45],\n",
       "       [ 155,  208,  952, ...,   70,  613,   31],\n",
       "       [2505,   54,    7, ...,  299,  428,   31]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloading Tuner from tuner_dir/mood_detection5/tuner0.json\n",
      "{'input_dim': 7000, 'output_dim': 32, 'num_layers': 2, 'units_layer1': 320, 'dropout_layer1': 0.30000000000000004, 'units_final': 384, 'l2_regularization': 0.0, 'kernel_initializer': 'glorot_uniform', 'optimizer': 'adam', 'learning_rate': 0.001, 'units_layer2': 32, 'dropout_layer2': 0.0}\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 3.4744 - valence_output_loss: 1.2052 - energy_output_loss: 1.2697 - danceability_output_loss: 0.9995 - valence_output_accuracy: 0.6274 - energy_output_accuracy: 0.6749 - danceability_output_accuracy: 0.7341\n",
      "Test Loss: 3.4744436740875244, valence_output_loss: 1.2052104473114014, energy_output_loss: 1.2696882486343384, danceability_output_loss: 0.9995446801185608, Test Accuracy Valence: 0.6274343729019165, Test Accuracy Energy: 0.6748518347740173, Test Accuracy Danceability: 0.7341236472129822\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner import RandomSearch, HyperParameters, Objective\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, Flatten\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# 构建模型函数\n",
    "def build_model(hp):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    x = Embedding(input_dim=hp.Int('input_dim', min_value=1000, max_value=10000, step=1000),\n",
    "                  output_dim=hp.Int('output_dim', min_value=32, max_value=128, step=32),\n",
    "                  input_length=max_len)(inputs)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    num_layers = hp.Int('num_layers', min_value=1, max_value=5, step=1)\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            x = Dense(units=hp.Int(f'units_layer{i+1}', min_value=32, max_value=512, step=32), activation='relu')(x)\n",
    "        else:\n",
    "            x = Dense(units=hp.Int(f'units_layer{i+1}', min_value=32, max_value=512, step=32), activation='relu')(x)\n",
    "        x = Dropout(rate=hp.Float(f'dropout_layer{i+1}', min_value=0.0, max_value=0.5, step=0.1))(x)\n",
    "\n",
    "    x = Dense(units=hp.Int('units_final', min_value=32, max_value=512, step=32),\n",
    "              activation='relu',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2_regularization', values=[0.0, 1e-4, 1e-3])),\n",
    "              kernel_initializer=hp.Choice('kernel_initializer', values=['glorot_uniform', 'he_normal']))(x)\n",
    "\n",
    "    \n",
    "    output_valence = Dense(y_valence.shape[1], activation='softmax', name='valence_output')(x)\n",
    "    output_energy = Dense(y_energy.shape[1], activation='softmax', name='energy_output')(x)\n",
    "    output_danceability = Dense(y_danceability.shape[1], activation='softmax', name='danceability_output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[output_valence, output_energy, output_danceability])\n",
    "\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss={'valence_output': 'categorical_crossentropy', \n",
    "                        'energy_output': 'categorical_crossentropy', \n",
    "                        'danceability_output': 'categorical_crossentropy'},\n",
    "                  metrics={'valence_output': 'accuracy', \n",
    "                           'energy_output': 'accuracy', \n",
    "                           'danceability_output': 'accuracy'})\n",
    "    return model\n",
    "\n",
    "# 超参数调优\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=Objective('val_valence_output_accuracy', direction='max'),\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='mood_detection5'\n",
    ")\n",
    "\n",
    "# 启动调优过程\n",
    "tuner.search(X_train, [y_train_valence, y_train_energy, y_train_danceability], \n",
    "             epochs=20, \n",
    "             validation_data=(X_val, [y_val_valence, y_val_energy, y_val_danceability]), \n",
    "             callbacks=[EarlyStopping(patience=3)])\n",
    "\n",
    "# 获取最佳模型\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hyperparameters.values)\n",
    "\n",
    "# 评估模型\n",
    "\n",
    "loss, valence_output_loss, energy_output_loss, danceability_output_loss, accuracy_valence, accuracy_energy, accuracy_danceability = best_model.evaluate(X_test, [y_test_valence, y_test_energy, y_test_danceability])\n",
    "print(f'Test Loss: {loss}, valence_output_loss: {valence_output_loss}, energy_output_loss: {energy_output_loss}, danceability_output_loss: {danceability_output_loss}, Test Accuracy Valence: {accuracy_valence}, Test Accuracy Energy: {accuracy_energy}, Test Accuracy Danceability: {accuracy_danceability}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 1ms/step - loss: 3.4744 - valence_output_loss: 1.2052 - energy_output_loss: 1.2697 - danceability_output_loss: 0.9995 - valence_output_accuracy: 0.6274 - energy_output_accuracy: 0.6749 - danceability_output_accuracy: 0.7341\n",
      "[3.4744436740875244, 1.2052104473114014, 1.2696882486343384, 0.9995446801185608, 0.6274343729019165, 0.6748518347740173, 0.7341236472129822]\n"
     ]
    }
   ],
   "source": [
    "metrics = best_model.evaluate(X_test, [y_test_valence, y_test_energy, y_test_danceability])\n",
    "#print(f'Test Loss: {loss}, Test Accuracy Valence: {accuracy_valence}, Test Accuracy Energy: {accuracy_energy}, Test Accuracy Danceability: {accuracy_danceability}')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 12s]\n",
      "val_valence_output_accuracy: 0.6536017060279846\n",
      "\n",
      "Best val_valence_output_accuracy So Far: 0.6546609997749329\n",
      "Total elapsed time: 00h 02m 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_output_dim': 32, 'filters': 96, 'kernel_size': 7, 'pool_size': 2, 'num_layers': 1, 'dense_units_1': 416, 'dropout_1': 0.4, 'optimizer': 'rmsprop', 'learning_rate': 0.001}\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 3.3224 - valence_output_loss: 1.1877 - energy_output_loss: 1.2021 - danceability_output_loss: 0.9325 - valence_output_accuracy: 0.6613 - energy_output_accuracy: 0.6520 - danceability_output_accuracy: 0.7722\n",
      "Test Loss: 3.3223869800567627, valence_output_loss: 1.187723159790039, energy_output_loss: 1.2021470069885254, danceability_output_loss: 0.9325172305107117, Test Accuracy Valence: 0.6613039970397949, Test Accuracy Energy: 0.6519898176193237, Test Accuracy Danceability: 0.7722269296646118\n"
     ]
    }
   ],
   "source": [
    "# 构建 CNN 模型函数\n",
    "def build_cnn_model(hp):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    x = Embedding(input_dim=max_words, output_dim=hp.Int('embedding_output_dim', min_value=32, max_value=128, step=32), input_length=max_len)(inputs)\n",
    "    x = tf.keras.layers.Conv1D(filters=hp.Int('filters', min_value=32, max_value=128, step=32), kernel_size=hp.Int('kernel_size', min_value=3, max_value=7, step=2), activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling1D(pool_size=hp.Int('pool_size', min_value=2, max_value=5, step=1))(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    num_layers = hp.Int('num_layers', min_value=1, max_value=3, step=1)\n",
    "    for i in range(num_layers):\n",
    "        x = Dense(units=hp.Int(f'dense_units_{i+1}', min_value=32, max_value=512, step=32), activation='relu')(x)\n",
    "        x = Dropout(rate=hp.Float(f'dropout_{i+1}', min_value=0.0, max_value=0.5, step=0.1))(x)\n",
    "    \n",
    "    output_valence = Dense(y_valence.shape[1], activation='softmax', name='valence_output')(x)\n",
    "    output_energy = Dense(y_energy.shape[1], activation='softmax', name='energy_output')(x)\n",
    "    output_danceability = Dense(y_danceability.shape[1], activation='softmax', name='danceability_output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[output_valence, output_energy, output_danceability])\n",
    "\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss={'valence_output': 'categorical_crossentropy', \n",
    "                        'energy_output': 'categorical_crossentropy', \n",
    "                        'danceability_output': 'categorical_crossentropy'},\n",
    "                  metrics={'valence_output': 'accuracy', \n",
    "                           'energy_output': 'accuracy', \n",
    "                           'danceability_output': 'accuracy'})\n",
    "    return model\n",
    "\n",
    "# 超参数调优\n",
    "tuner = RandomSearch(\n",
    "    build_cnn_model,\n",
    "    objective=Objective('val_valence_output_accuracy', direction='max'),\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='cnn_mood_detection2'\n",
    ")\n",
    "\n",
    "# 启动调优过程\n",
    "tuner.search(X_train, [y_train_valence, y_train_energy, y_train_danceability], \n",
    "             epochs=20, \n",
    "             validation_data=(X_val, [y_val_valence, y_val_energy, y_val_danceability]), \n",
    "             callbacks=[EarlyStopping(patience=3)])\n",
    "\n",
    "# 获取最佳模型\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hyperparameters.values)\n",
    "\n",
    "# 评估模型\n",
    "loss, valence_output_loss, energy_output_loss, danceability_output_loss, accuracy_valence, accuracy_energy, accuracy_danceability = best_model.evaluate(X_test, [y_test_valence, y_test_energy, y_test_danceability])\n",
    "print(f'Test Loss: {loss}, valence_output_loss: {valence_output_loss}, energy_output_loss: {energy_output_loss}, danceability_output_loss: {danceability_output_loss}, Test Accuracy Valence: {accuracy_valence}, Test Accuracy Energy: {accuracy_energy}, Test Accuracy Danceability: {accuracy_danceability}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.iter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.beta_2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer.learning_rate\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2674: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input [229 582 229 582 229 266  15   3 249 580  27 314   2 568  59 669 329   2\n   2 249 580  27 314 568  59 669 329   2   2   2   2 266 266 266 266 266\n 266 266 266 266 266 266 266 266 266 117 117 117 117  63 117 117 146 119\n 119 582 229 582 229 582 229 266  63 117 117 146 119 119 582 229 582 229\n 582 229 266  63 117 117 117 146 119  63 117 117 117 146 119  63 117 117\n 117 146 119  63 117 117 117 146 119  31] is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(input_ids), np\u001b[38;5;241m.\u001b[39marray(attention_masks)\n\u001b[1;32m     21\u001b[0m max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m---> 23\u001b[0m X_train_input_ids, X_train_attention_masks \u001b[38;5;241m=\u001b[39m \u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m X_val_input_ids, X_val_attention_masks \u001b[38;5;241m=\u001b[39m tokenize(X_val, tokenizer, max_len)\n\u001b[1;32m     25\u001b[0m X_test_input_ids, X_test_attention_masks \u001b[38;5;241m=\u001b[39m tokenize(X_test, tokenizer, max_len)\n",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m, in \u001b[0;36mtokenize\u001b[0;34m(sentences, tokenizer, max_len)\u001b[0m\n\u001b[1;32m      6\u001b[0m input_ids, attention_masks \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[0;32m----> 8\u001b[0m     encoded \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_max_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     input_ids\u001b[38;5;241m.\u001b[39mappend(encoded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     18\u001b[0m     attention_masks\u001b[38;5;241m.\u001b[39mappend(encoded[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3037\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3028\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3029\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   3030\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3034\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   3035\u001b[0m )\n\u001b[0;32m-> 3037\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   3038\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   3039\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   3040\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   3041\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   3042\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   3043\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   3044\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   3045\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   3046\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   3047\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   3048\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   3049\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   3050\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   3051\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   3052\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   3053\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   3054\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   3055\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   3056\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils.py:719\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    711\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    712\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    713\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m     )\n\u001b[0;32m--> 719\u001b[0m first_ids \u001b[39m=\u001b[39m get_input_ids(text)\n\u001b[1;32m    720\u001b[0m second_ids \u001b[39m=\u001b[39m get_input_ids(text_pair) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_for_model(\n\u001b[1;32m    723\u001b[0m     first_ids,\n\u001b[1;32m    724\u001b[0m     pair_ids\u001b[39m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    738\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m    739\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/tokenization_utils.py:705\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    701\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string or a list/tuple of strings when\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    702\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `is_split_into_words=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    703\u001b[0m     )\n\u001b[1;32m    704\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    706\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput \u001b[39m\u001b[39m{\u001b[39;00mtext\u001b[39m}\u001b[39;00m\u001b[39m is not valid. Should be a string, a list/tuple of strings or a list/tuple of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    707\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m integers.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    708\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Input [229 582 229 582 229 266  15   3 249 580  27 314   2 568  59 669 329   2\n   2 249 580  27 314 568  59 669 329   2   2   2   2 266 266 266 266 266\n 266 266 266 266 266 266 266 266 266 117 117 117 117  63 117 117 146 119\n 119 582 229 582 229 582 229 266  63 117 117 146 119 119 582 229 582 229\n 582 229 266  63 117 117 117 146 119  63 117 117 117 146 119  63 117 117\n 117 146 119  63 117 117 117 146 119  31] is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers."
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "# 使用 BertTokenizer 和 TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(sentences, tokenizer, max_len=128):\n",
    "    input_ids, attention_masks = [], []\n",
    "    for sent in sentences:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text=sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "max_len = 128\n",
    "\n",
    "X_train_input_ids, X_train_attention_masks = tokenize(data['lyrics'], tokenizer, max_len)\n",
    "#X_val_input_ids, X_val_attention_masks = tokenize(X_val, tokenizer, max_len)\n",
    "#X_test_input_ids, X_test_attention_masks = tokenize(X_test, tokenizer, max_len)\n",
    "\n",
    "# 构建BERT模型\n",
    "def build_bert_model():\n",
    "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
    "    cls_token = bert_output[:, 0, :]\n",
    "    \n",
    "    dense_valence = Dense(y_valence.shape[1], activation='softmax', name='valence_output')(cls_token)\n",
    "    dense_energy = Dense(y_energy.shape[1], activation='softmax', name='energy_output')(cls_token)\n",
    "    dense_danceability = Dense(y_danceability.shape[1], activation='softmax', name='danceability_output')(cls_token)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=[dense_valence, dense_energy, dense_danceability])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=2e-5)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss={'valence_output': 'categorical_crossentropy', \n",
    "                        'energy_output': 'categorical_crossentropy', \n",
    "                        'danceability_output': 'categorical_crossentropy'},\n",
    "                  metrics={'valence_output': 'accuracy', \n",
    "                           'energy_output': 'accuracy', \n",
    "                           'danceability_output': 'accuracy'})\n",
    "    return model\n",
    "\n",
    "# 构建并训练模型\n",
    "model = build_bert_model()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_input_ids, X_train_attention_masks],\n",
    "    [y_train_valence, y_train_energy, y_train_danceability],\n",
    "    validation_data=([X_val_input_ids, X_val_attention_masks], [y_val_valence, y_val_energy, y_val_danceability]),\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# 评估模型\n",
    "loss, valence_output_loss, energy_output_loss, danceability_output_loss, accuracy_valence, accuracy_energy, accuracy_danceability = model.evaluate(\n",
    "    [X_test_input_ids, X_test_attention_masks], \n",
    "    [y_test_valence, y_test_energy, y_test_danceability]\n",
    ")\n",
    "\n",
    "print(f'Test Loss: {loss}, valence_output_loss: {valence_output_loss}, energy_output_loss: {energy_output_loss}, danceability_output_loss: {danceability_output_loss}')\n",
    "print(f'Test Accuracy Valence: {accuracy_valence}, Test Accuracy Energy: {accuracy_energy}, Test Accuracy Danceability: {accuracy_danceability}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
