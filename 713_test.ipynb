{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "from nltk.corpus import stopwords\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras_tuner import RandomSearch, HyperParameters, Objective\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, Layer\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, Callback\n",
    "from transformers import BertTokenizer, TFBertModel, get_linear_schedule_with_warmup, WarmUp, AdamW, RobertaTokenizer, TFRobertaModel, RobertaModel, XLMRobertaTokenizer, XLMRobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 解压 cleaned_lyrics.zip 文件\n",
    "with zipfile.ZipFile('sampled.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall('sampled')\n",
    "\n",
    "# 获取所有歌词文件的路径\n",
    "lyrics_files = {os.path.splitext(f)[0]: os.path.join('sampled', f) for f in os.listdir('sampled')}\n",
    "\n",
    "# 读取 filtered_dataset.csv 文件\n",
    "data = pd.read_csv('sampled_dataset.csv')\n",
    "\n",
    "def read_lyrics(record_id):\n",
    "    file_path = lyrics_files.get(str(record_id))\n",
    "    if file_path and os.path.exists(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    return ''\n",
    "\n",
    "# 读取歌词并添加到数据框中\n",
    "data['lyrics'] = data['record_id'].apply(read_lyrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0.2</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>record_id</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>track_id</th>\n",
       "      <th>artists</th>\n",
       "      <th>album_name</th>\n",
       "      <th>track_name</th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "      <th>track_genre</th>\n",
       "      <th>valence_bin</th>\n",
       "      <th>energy_bin</th>\n",
       "      <th>danceability_bin</th>\n",
       "      <th>lyrics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1458</td>\n",
       "      <td>2834</td>\n",
       "      <td>2834</td>\n",
       "      <td>20097</td>\n",
       "      <td>4b98LXC0QUWGBteJ5uwVQY</td>\n",
       "      <td>Doja Cat</td>\n",
       "      <td>Summer Music Festival Hits</td>\n",
       "      <td>Boss Bitch</td>\n",
       "      <td>0</td>\n",
       "      <td>134239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.575</td>\n",
       "      <td>125.993</td>\n",
       "      <td>4</td>\n",
       "      <td>dance</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>mmm not tryna ah not tryna not tryna yeah not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1557</td>\n",
       "      <td>2967</td>\n",
       "      <td>2967</td>\n",
       "      <td>22816</td>\n",
       "      <td>58Z83tSbShyHxCwqTCE8M6</td>\n",
       "      <td>Goatwhore</td>\n",
       "      <td>Vengeful Ascension</td>\n",
       "      <td>Under the Flesh, Into the Soul</td>\n",
       "      <td>21</td>\n",
       "      <td>273266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.347</td>\n",
       "      <td>170.015</td>\n",
       "      <td>4</td>\n",
       "      <td>death-metal</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>world grave apathetic cold selfish prison cove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>425</td>\n",
       "      <td>616</td>\n",
       "      <td>616</td>\n",
       "      <td>3220</td>\n",
       "      <td>0AOmbw8AwDnwXhHC3OhdVB</td>\n",
       "      <td>Thousand Foot Krutch</td>\n",
       "      <td>The End Is Where We Begin</td>\n",
       "      <td>Courtesy Call</td>\n",
       "      <td>72</td>\n",
       "      <td>236898</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0822</td>\n",
       "      <td>0.445</td>\n",
       "      <td>164.079</td>\n",
       "      <td>4</td>\n",
       "      <td>alternative</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>hey comes danger club get started man not gonn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>989</td>\n",
       "      <td>2054</td>\n",
       "      <td>2054</td>\n",
       "      <td>14581</td>\n",
       "      <td>5Jaj6nLjHCizmcPddJLO3k</td>\n",
       "      <td>Blippi</td>\n",
       "      <td>Blippi Tunes, Vol. 2: Machines (Music for Todd...</td>\n",
       "      <td>The Train Song</td>\n",
       "      <td>53</td>\n",
       "      <td>207428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.3250</td>\n",
       "      <td>0.662</td>\n",
       "      <td>139.895</td>\n",
       "      <td>4</td>\n",
       "      <td>children</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>choo choo comes train choo choo comes train ro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>383</td>\n",
       "      <td>562</td>\n",
       "      <td>562</td>\n",
       "      <td>3717</td>\n",
       "      <td>2oaK4JLVnmRGIO9ytBE1bt</td>\n",
       "      <td>Red Hot Chili Peppers</td>\n",
       "      <td>The Getaway</td>\n",
       "      <td>Dark Necessities</td>\n",
       "      <td>74</td>\n",
       "      <td>302000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019900</td>\n",
       "      <td>0.1100</td>\n",
       "      <td>0.197</td>\n",
       "      <td>91.959</td>\n",
       "      <td>4</td>\n",
       "      <td>alternative</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>comin light day got many moons deep play keep ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>658</td>\n",
       "      <td>1229</td>\n",
       "      <td>1229</td>\n",
       "      <td>8662</td>\n",
       "      <td>3hSy2AUoElgIk6fjhYnRH3</td>\n",
       "      <td>Elvin Bishop</td>\n",
       "      <td>Sure Feels Good: The Best Of Elvin Bishop</td>\n",
       "      <td>Fooled Around And Fell In Love</td>\n",
       "      <td>57</td>\n",
       "      <td>276933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.0953</td>\n",
       "      <td>0.610</td>\n",
       "      <td>113.463</td>\n",
       "      <td>3</td>\n",
       "      <td>blues</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>million girls love em leave em alone not care ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>5818</td>\n",
       "      <td>10682</td>\n",
       "      <td>10682</td>\n",
       "      <td>113479</td>\n",
       "      <td>5ELZpvTDGorz9BIE9zaBoZ</td>\n",
       "      <td>Tenth Avenue North</td>\n",
       "      <td>Followers</td>\n",
       "      <td>I Have This Hope</td>\n",
       "      <td>52</td>\n",
       "      <td>204800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1890</td>\n",
       "      <td>0.110</td>\n",
       "      <td>108.009</td>\n",
       "      <td>4</td>\n",
       "      <td>world-music</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>walk great unknown questions come questions go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>265</td>\n",
       "      <td>426</td>\n",
       "      <td>426</td>\n",
       "      <td>2185</td>\n",
       "      <td>3mJV4kByjmgU3ubU7JPp9W</td>\n",
       "      <td>Marilyn Manson</td>\n",
       "      <td>Halloween 2022</td>\n",
       "      <td>You And Me And The Devil Makes 3</td>\n",
       "      <td>0</td>\n",
       "      <td>264266</td>\n",
       "      <td>...</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>0.2030</td>\n",
       "      <td>0.399</td>\n",
       "      <td>128.021</td>\n",
       "      <td>4</td>\n",
       "      <td>alt-rock</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>like rolling stone hill hades want lie gonna l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>5031</td>\n",
       "      <td>9325</td>\n",
       "      <td>9325</td>\n",
       "      <td>94717</td>\n",
       "      <td>5xUpsNCW71S58c8TycsqNa</td>\n",
       "      <td>Ollie</td>\n",
       "      <td>Sunsets &amp; Goodbyes</td>\n",
       "      <td>what if</td>\n",
       "      <td>39</td>\n",
       "      <td>173615</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.0723</td>\n",
       "      <td>0.283</td>\n",
       "      <td>146.006</td>\n",
       "      <td>4</td>\n",
       "      <td>sad</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>yeah call one day everything yeah call next no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>5723</td>\n",
       "      <td>10541</td>\n",
       "      <td>10541</td>\n",
       "      <td>107316</td>\n",
       "      <td>7skyR8nK3vnDikYFBoUVw6</td>\n",
       "      <td>Laura Branigan</td>\n",
       "      <td>Self Control (Expanded)</td>\n",
       "      <td>Self Control - Extended Version</td>\n",
       "      <td>52</td>\n",
       "      <td>304893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009970</td>\n",
       "      <td>0.1880</td>\n",
       "      <td>0.803</td>\n",
       "      <td>106.378</td>\n",
       "      <td>4</td>\n",
       "      <td>synth-pop</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>oh night world city light painted girl day not...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0.2  Unnamed: 0.1  record_id  Unnamed: 0  \\\n",
       "0            1458          2834       2834       20097   \n",
       "1            1557          2967       2967       22816   \n",
       "2             425           616        616        3220   \n",
       "3             989          2054       2054       14581   \n",
       "4             383           562        562        3717   \n",
       "..            ...           ...        ...         ...   \n",
       "595           658          1229       1229        8662   \n",
       "596          5818         10682      10682      113479   \n",
       "597           265           426        426        2185   \n",
       "598          5031          9325       9325       94717   \n",
       "599          5723         10541      10541      107316   \n",
       "\n",
       "                   track_id                artists  \\\n",
       "0    4b98LXC0QUWGBteJ5uwVQY               Doja Cat   \n",
       "1    58Z83tSbShyHxCwqTCE8M6              Goatwhore   \n",
       "2    0AOmbw8AwDnwXhHC3OhdVB   Thousand Foot Krutch   \n",
       "3    5Jaj6nLjHCizmcPddJLO3k                 Blippi   \n",
       "4    2oaK4JLVnmRGIO9ytBE1bt  Red Hot Chili Peppers   \n",
       "..                      ...                    ...   \n",
       "595  3hSy2AUoElgIk6fjhYnRH3           Elvin Bishop   \n",
       "596  5ELZpvTDGorz9BIE9zaBoZ     Tenth Avenue North   \n",
       "597  3mJV4kByjmgU3ubU7JPp9W         Marilyn Manson   \n",
       "598  5xUpsNCW71S58c8TycsqNa                  Ollie   \n",
       "599  7skyR8nK3vnDikYFBoUVw6         Laura Branigan   \n",
       "\n",
       "                                            album_name  \\\n",
       "0                           Summer Music Festival Hits   \n",
       "1                                   Vengeful Ascension   \n",
       "2                            The End Is Where We Begin   \n",
       "3    Blippi Tunes, Vol. 2: Machines (Music for Todd...   \n",
       "4                                          The Getaway   \n",
       "..                                                 ...   \n",
       "595          Sure Feels Good: The Best Of Elvin Bishop   \n",
       "596                                          Followers   \n",
       "597                                     Halloween 2022   \n",
       "598                                 Sunsets & Goodbyes   \n",
       "599                            Self Control (Expanded)   \n",
       "\n",
       "                           track_name  popularity  duration_ms  ...  \\\n",
       "0                          Boss Bitch           0       134239  ...   \n",
       "1      Under the Flesh, Into the Soul          21       273266  ...   \n",
       "2                       Courtesy Call          72       236898  ...   \n",
       "3                      The Train Song          53       207428  ...   \n",
       "4                    Dark Necessities          74       302000  ...   \n",
       "..                                ...         ...          ...  ...   \n",
       "595    Fooled Around And Fell In Love          57       276933  ...   \n",
       "596                  I Have This Hope          52       204800  ...   \n",
       "597  You And Me And The Devil Makes 3           0       264266  ...   \n",
       "598                           what if          39       173615  ...   \n",
       "599   Self Control - Extended Version          52       304893  ...   \n",
       "\n",
       "     instrumentalness  liveness  valence    tempo  time_signature  \\\n",
       "0            0.000000    0.2030    0.575  125.993               4   \n",
       "1            0.110000    0.1070    0.347  170.015               4   \n",
       "2            0.000000    0.0822    0.445  164.079               4   \n",
       "3            0.000019    0.3250    0.662  139.895               4   \n",
       "4            0.019900    0.1100    0.197   91.959               4   \n",
       "..                ...       ...      ...      ...             ...   \n",
       "595          0.080500    0.0953    0.610  113.463               3   \n",
       "596          0.000000    0.1890    0.110  108.009               4   \n",
       "597          0.834000    0.2030    0.399  128.021               4   \n",
       "598          0.000627    0.0723    0.283  146.006               4   \n",
       "599          0.009970    0.1880    0.803  106.378               4   \n",
       "\n",
       "     track_genre  valence_bin  energy_bin  danceability_bin  \\\n",
       "0          dance            1           2                 2   \n",
       "1    death-metal            1           2                 0   \n",
       "2    alternative            1           1                 1   \n",
       "3       children            2           1                 2   \n",
       "4    alternative            0           2                 2   \n",
       "..           ...          ...         ...               ...   \n",
       "595        blues            1           1                 1   \n",
       "596  world-music            0           1                 1   \n",
       "597     alt-rock            1           2                 1   \n",
       "598          sad            0           0                 2   \n",
       "599    synth-pop            2           2                 2   \n",
       "\n",
       "                                                lyrics  \n",
       "0    mmm not tryna ah not tryna not tryna yeah not ...  \n",
       "1    world grave apathetic cold selfish prison cove...  \n",
       "2    hey comes danger club get started man not gonn...  \n",
       "3    choo choo comes train choo choo comes train ro...  \n",
       "4    comin light day got many moons deep play keep ...  \n",
       "..                                                 ...  \n",
       "595  million girls love em leave em alone not care ...  \n",
       "596  walk great unknown questions come questions go...  \n",
       "597  like rolling stone hill hades want lie gonna l...  \n",
       "598  yeah call one day everything yeah call next no...  \n",
       "599  oh night world city light painted girl day not...  \n",
       "\n",
       "[600 rows x 28 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 Tokenizer 处理文本\n",
    "max_words = 5000\n",
    "max_len = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(data['lyrics'])\n",
    "sequences = tokenizer.texts_to_sequences(data['lyrics'])\n",
    "X_lyrics = pad_sequences(sequences, maxlen=max_len)\n",
    "\n",
    "# 准备标签\n",
    "y_valence = to_categorical(data['valence_bin'].values)\n",
    "y_energy = to_categorical(data['energy_bin'].values)\n",
    "y_danceability = to_categorical(data['danceability_bin'].values)\n",
    "\n",
    "# 拆分数据集\n",
    "X_train_val, X_test, y_train_val_valence, y_test_valence, y_train_val_energy, y_test_energy, y_train_val_danceability, y_test_danceability = train_test_split(\n",
    "    X_lyrics, y_valence, y_energy, y_danceability, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train_valence, y_val_valence, y_train_energy, y_val_energy, y_train_danceability, y_val_danceability = train_test_split(\n",
    "    X_train_val, y_train_val_valence, y_train_val_energy, y_train_val_danceability, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   9,   80,   77, ...,  718, 3619,   32],\n",
       "       [  28,  344,  780, ...,  344,  780,   32],\n",
       "       [  77,  609,  198, ...,  609,  198,   32],\n",
       "       ...,\n",
       "       [   1,    1,  433, ...,   28,   44,   32],\n",
       "       [ 653, 1380,  591, ...,    4,  203,   32],\n",
       "       [  58,  135,   13, ..., 1744,  313,   32]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 03s]\n",
      "val_valence_output_accuracy: 0.5416666865348816\n",
      "\n",
      "Best val_valence_output_accuracy So Far: 0.5416666865348816\n",
      "Total elapsed time: 00h 00m 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 24 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_dim': 7000, 'output_dim': 128, 'num_layers': 1, 'units_layer1': 32, 'dropout_layer1': 0.0, 'units_final': 256, 'l2_regularization': 0.0, 'kernel_initializer': 'glorot_uniform', 'optimizer': 'adam', 'learning_rate': 0.0001, 'units_layer2': 448, 'dropout_layer2': 0.0, 'units_layer3': 160, 'dropout_layer3': 0.4, 'units_layer4': 416, 'dropout_layer4': 0.30000000000000004, 'units_layer5': 384, 'dropout_layer5': 0.2}\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - danceability_output_accuracy: 0.4948 - energy_output_accuracy: 0.3417 - loss: 3.1083 - valence_output_accuracy: 0.3975  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 77\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_hyperparameters\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m loss, valence_output_loss, energy_output_loss, danceability_output_loss, accuracy_valence, accuracy_energy, accuracy_danceability \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mevaluate(X_test, [y_test_valence, y_test_energy, y_test_danceability])\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, valence_output_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalence_output_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, energy_output_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menergy_output_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, danceability_output_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdanceability_output_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy Valence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_valence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy Energy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_energy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy Danceability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_danceability\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 4)"
     ]
    }
   ],
   "source": [
    "from keras_tuner import RandomSearch, HyperParameters, Objective\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Dropout, Embedding, Flatten\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "# 构建模型函数\n",
    "def build_model(hp):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    x = Embedding(input_dim=hp.Int('input_dim', min_value=1000, max_value=10000, step=1000),\n",
    "                  output_dim=hp.Int('output_dim', min_value=32, max_value=128, step=32),\n",
    "                  input_length=max_len)(inputs)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    num_layers = hp.Int('num_layers', min_value=1, max_value=5, step=1)\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            x = Dense(units=hp.Int(f'units_layer{i+1}', min_value=32, max_value=512, step=32), activation='relu')(x)\n",
    "        else:\n",
    "            x = Dense(units=hp.Int(f'units_layer{i+1}', min_value=32, max_value=512, step=32), activation='relu')(x)\n",
    "        x = Dropout(rate=hp.Float(f'dropout_layer{i+1}', min_value=0.0, max_value=0.5, step=0.1))(x)\n",
    "\n",
    "    x = Dense(units=hp.Int('units_final', min_value=32, max_value=512, step=32),\n",
    "              activation='relu',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(hp.Choice('l2_regularization', values=[0.0, 1e-4, 1e-3])),\n",
    "              kernel_initializer=hp.Choice('kernel_initializer', values=['glorot_uniform', 'he_normal']))(x)\n",
    "\n",
    "    \n",
    "    output_valence = Dense(y_valence.shape[1], activation='softmax', name='valence_output')(x)\n",
    "    output_energy = Dense(y_energy.shape[1], activation='softmax', name='energy_output')(x)\n",
    "    output_danceability = Dense(y_danceability.shape[1], activation='softmax', name='danceability_output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[output_valence, output_energy, output_danceability])\n",
    "\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss={'valence_output': 'categorical_crossentropy', \n",
    "                        'energy_output': 'categorical_crossentropy', \n",
    "                        'danceability_output': 'categorical_crossentropy'},\n",
    "                  metrics={'valence_output': 'accuracy', \n",
    "                           'energy_output': 'accuracy', \n",
    "                           'danceability_output': 'accuracy'})\n",
    "    return model\n",
    "\n",
    "# 超参数调优\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective=Objective('val_valence_output_accuracy', direction='max'),\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='dnn_mood_detection_600'\n",
    ")\n",
    "\n",
    "# 启动调优过程\n",
    "tuner.search(X_train, [y_train_valence, y_train_energy, y_train_danceability], \n",
    "             epochs=20, \n",
    "             validation_data=(X_val, [y_val_valence, y_val_energy, y_val_danceability]), \n",
    "             callbacks=[EarlyStopping(patience=3)])\n",
    "\n",
    "# 获取最佳模型\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hyperparameters.values)\n",
    "\n",
    "# 评估模型\n",
    "\n",
    "loss, valence_output_loss, energy_output_loss, danceability_output_loss, accuracy_valence, accuracy_energy, accuracy_danceability = best_model.evaluate(X_test, [y_test_valence, y_test_energy, y_test_danceability])\n",
    "print(f'Test Loss: {loss}, valence_output_loss: {valence_output_loss}, energy_output_loss: {energy_output_loss}, danceability_output_loss: {danceability_output_loss}, Test Accuracy Valence: {accuracy_valence}, Test Accuracy Energy: {accuracy_energy}, Test Accuracy Danceability: {accuracy_danceability}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - danceability_output_accuracy: 0.4948 - energy_output_accuracy: 0.3417 - loss: 3.1083 - valence_output_accuracy: 0.3975 \n",
      "[3.0829503536224365, 0.5, 0.375, 0.4000000059604645]\n"
     ]
    }
   ],
   "source": [
    "metrics = best_model.evaluate(X_test, [y_test_valence, y_test_energy, y_test_danceability])\n",
    "#print(f'Test Loss: {loss}, Test Accuracy Valence: {accuracy_valence}, Test Accuracy Energy: {accuracy_energy}, Test Accuracy Danceability: {accuracy_danceability}')\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 02s]\n",
      "val_valence_output_accuracy: 0.4791666567325592\n",
      "\n",
      "Best val_valence_output_accuracy So Far: 0.53125\n",
      "Total elapsed time: 00h 00m 26s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'rmsprop', because it has 2 variables whereas the saved optimizer has 17 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_output_dim': 64, 'filters': 64, 'kernel_size': 3, 'pool_size': 5, 'num_layers': 3, 'dense_units_1': 96, 'dropout_1': 0.4, 'optimizer': 'rmsprop', 'learning_rate': 0.001, 'dense_units_2': 32, 'dropout_2': 0.0, 'dense_units_3': 32, 'dropout_3': 0.0}\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - danceability_output_accuracy: 0.4258 - energy_output_accuracy: 0.3546 - loss: 3.1556 - valence_output_accuracy: 0.4721  \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 61\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(best_hyperparameters\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# 评估模型\u001b[39;00m\n\u001b[1;32m---> 61\u001b[0m loss, valence_output_loss, energy_output_loss, danceability_output_loss, accuracy_valence, accuracy_energy, accuracy_danceability \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mevaluate(X_test, [y_test_valence, y_test_energy, y_test_danceability])\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, valence_output_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalence_output_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, energy_output_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menergy_output_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, danceability_output_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdanceability_output_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy Valence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_valence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy Energy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_energy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy Danceability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_danceability\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 4)"
     ]
    }
   ],
   "source": [
    "# 构建 CNN 模型函数\n",
    "def build_cnn_model(hp):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    x = Embedding(input_dim=max_words, output_dim=hp.Int('embedding_output_dim', min_value=32, max_value=128, step=32), input_length=max_len)(inputs)\n",
    "    x = tf.keras.layers.Conv1D(filters=hp.Int('filters', min_value=32, max_value=128, step=32), kernel_size=hp.Int('kernel_size', min_value=3, max_value=7, step=2), activation='relu')(x)\n",
    "    x = tf.keras.layers.AveragePooling1D(pool_size=hp.Int('pool_size', min_value=2, max_value=5, step=1))(x)\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    num_layers = hp.Int('num_layers', min_value=1, max_value=3, step=1)\n",
    "    for i in range(num_layers):\n",
    "        x = Dense(units=hp.Int(f'dense_units_{i+1}', min_value=32, max_value=512, step=32), activation='relu')(x)\n",
    "        x = Dropout(rate=hp.Float(f'dropout_{i+1}', min_value=0.0, max_value=0.5, step=0.1))(x)\n",
    "    \n",
    "    output_valence = Dense(y_valence.shape[1], activation='softmax', name='valence_output')(x)\n",
    "    output_energy = Dense(y_energy.shape[1], activation='softmax', name='energy_output')(x)\n",
    "    output_danceability = Dense(y_danceability.shape[1], activation='softmax', name='danceability_output')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=[output_valence, output_energy, output_danceability])\n",
    "\n",
    "    optimizer_choice = hp.Choice('optimizer', values=['adam', 'rmsprop', 'sgd'])\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-3, 1e-4, 1e-5])\n",
    "\n",
    "    if optimizer_choice == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_choice == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss={'valence_output': 'categorical_crossentropy', \n",
    "                        'energy_output': 'categorical_crossentropy', \n",
    "                        'danceability_output': 'categorical_crossentropy'},\n",
    "                  metrics={'valence_output': 'accuracy', \n",
    "                           'energy_output': 'accuracy', \n",
    "                           'danceability_output': 'accuracy'})\n",
    "    return model\n",
    "\n",
    "# 超参数调优\n",
    "tuner = RandomSearch(\n",
    "    build_cnn_model,\n",
    "    objective=Objective('val_valence_output_accuracy', direction='max'),\n",
    "    max_trials=10,\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_dir',\n",
    "    project_name='cnn_mood_detection_600'\n",
    ")\n",
    "\n",
    "# 启动调优过程\n",
    "tuner.search(X_train, [y_train_valence, y_train_energy, y_train_danceability], \n",
    "             epochs=20, \n",
    "             validation_data=(X_val, [y_val_valence, y_val_energy, y_val_danceability]), \n",
    "             callbacks=[EarlyStopping(patience=3)])\n",
    "\n",
    "# 获取最佳模型\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hyperparameters.values)\n",
    "\n",
    "# 评估模型\n",
    "loss, valence_output_loss, energy_output_loss, danceability_output_loss, accuracy_valence, accuracy_energy, accuracy_danceability = best_model.evaluate(X_test, [y_test_valence, y_test_energy, y_test_danceability])\n",
    "print(f'Test Loss: {loss}, valence_output_loss: {valence_output_loss}, energy_output_loss: {energy_output_loss}, danceability_output_loss: {danceability_output_loss}, Test Accuracy Valence: {accuracy_valence}, Test Accuracy Energy: {accuracy_energy}, Test Accuracy Danceability: {accuracy_danceability}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拆分数据集\n",
    "X_train_val, X_test, y_train_val_valence, y_test_valence, y_train_val_energy, y_test_energy, y_train_val_danceability, y_test_danceability = train_test_split(\n",
    "    data['lyrics'], y_valence, y_energy, y_danceability, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, X_val, y_train_valence, y_val_valence, y_train_energy, y_val_energy, y_train_danceability, y_val_danceability = train_test_split(\n",
    "    X_train_val, y_train_val_valence, y_train_val_energy, y_train_val_danceability, test_size=0.2, random_state=42)\n",
    "\n",
    "# 使用 BertTokenizer 和 TFBertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize(sentences, tokenizer, max_len=128):\n",
    "    input_ids, attention_masks = [], []\n",
    "    for sent in sentences:\n",
    "        encoded = tokenizer.encode_plus(\n",
    "            text=sent,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=False,\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoded['input_ids'])\n",
    "        attention_masks.append(encoded['attention_mask'])\n",
    "    return np.array(input_ids), np.array(attention_masks)\n",
    "\n",
    "max_len = 128\n",
    "\n",
    "X_train_input_ids, X_train_attention_masks = tokenize(X_train, tokenizer, max_len)\n",
    "X_val_input_ids, X_val_attention_masks = tokenize(X_val, tokenizer, max_len)\n",
    "X_test_input_ids, X_test_attention_masks = tokenize(X_test, tokenizer, max_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import transformers\n",
    "import torch\n",
    "import optuna\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn import metrics\n",
    "from transformers import BertForSequenceClassification, BertModel, get_linear_schedule_with_warmup, logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101,  2371,  5223, ...,  5223,  4125,   102],\n",
       "       [  101,  2821,  2132, ...,  8072,  2823,   102],\n",
       "       [  101,  2111,  2088, ...,  4536,  4536,   102],\n",
       "       ...,\n",
       "       [  101,  2092,  2387, ..., 12927,  2100,   102],\n",
       "       [  101,  2034,  3058, ...,  5236,  4845,   102],\n",
       "       [  101,  2379,  2203, ...,  5223,  8843,   102]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 3\n",
    "\n",
    "class MultiLabelBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_labels_valence, num_labels_energy, num_labels_danceability, dropout_rate):\n",
    "        super(MultiLabelBERT, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.classifier_valence = nn.Linear(self.bert.config.hidden_size, num_labels_valence)\n",
    "        self.classifier_energy = nn.Linear(self.bert.config.hidden_size, num_labels_energy)\n",
    "        self.classifier_danceability = nn.Linear(self.bert.config.hidden_size, num_labels_danceability)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels_valence=None, labels_energy=None, labels_danceability=None):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "        pooled_output = self.dropout(outputs.pooler_output)\n",
    "        \n",
    "        logits_valence = self.classifier_valence(pooled_output)\n",
    "        logits_energy = self.classifier_energy(pooled_output)\n",
    "        logits_danceability = self.classifier_danceability(pooled_output)\n",
    "        \n",
    "        loss = None\n",
    "        if labels_valence is not None and labels_energy is not None and labels_danceability is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss_valence = loss_fct(logits_valence, labels_valence)\n",
    "            loss_energy = loss_fct(logits_energy, labels_energy)\n",
    "            loss_danceability = loss_fct(logits_danceability, labels_danceability)\n",
    "            loss = loss_valence + loss_energy + loss_danceability\n",
    "        \n",
    "        return (loss, logits_valence, logits_energy, logits_danceability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_masks, labels_valence, labels_energy, labels_danceability):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_masks = attention_masks\n",
    "        self.labels_valence = np.argmax(labels_valence, axis=1)\n",
    "        self.labels_energy = np.argmax(labels_energy, axis=1)\n",
    "        self.labels_danceability = np.argmax(labels_danceability, axis=1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': torch.tensor(self.input_ids[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(self.attention_masks[idx], dtype=torch.long),\n",
    "            'labels_valence': torch.tensor(self.labels_valence[idx], dtype=torch.long),\n",
    "            'labels_energy': torch.tensor(self.labels_energy[idx], dtype=torch.long),\n",
    "            'labels_danceability': torch.tensor(self.labels_danceability[idx], dtype=torch.long)\n",
    "        }\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(params):\n",
    "    lr = params['lr']\n",
    "    num_epochs = int(params['num_epochs'])\n",
    "    batch_size = int(params['batch_size'])\n",
    "    weight_decay = params['weight_decay']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MultiLabelBERT('bert-base-uncased', num_labels_valence=3, num_labels_energy=3, num_labels_danceability=3, dropout_rate=dropout_rate).to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_dataset = MultiLabelDataset(X_train_input_ids, X_train_attention_masks, y_train_valence, y_train_energy, y_train_danceability)\n",
    "    val_dataset = MultiLabelDataset(X_val_input_ids, X_val_attention_masks, y_val_valence, y_val_energy, y_val_danceability)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    num_training_steps = len(train_dataloader) * num_epochs\n",
    "    warmup_steps = int(0.1 * num_training_steps)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=num_training_steps)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = float(0)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions_valence = []\n",
    "        train_predictions_energy = []\n",
    "        train_predictions_danceability = []\n",
    "        train_labels_valence = []\n",
    "        train_labels_energy = []\n",
    "        train_labels_danceability = []\n",
    "        progress_bar = tqdm(train_dataloader, desc='Training', leave=False)\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_valence = batch['labels_valence'].to(device)\n",
    "            labels_energy = batch['labels_energy'].to(device)\n",
    "            labels_danceability = batch['labels_danceability'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels_valence=labels_valence, labels_energy=labels_energy, labels_danceability=labels_danceability)\n",
    "            loss, logits_valence, logits_energy, logits_danceability = outputs\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_predictions_valence.extend(logits_valence.argmax(dim=-1).cpu().numpy())\n",
    "            train_predictions_energy.extend(logits_energy.argmax(dim=-1).cpu().numpy())\n",
    "            train_predictions_danceability.extend(logits_danceability.argmax(dim=-1).cpu().numpy())\n",
    "            train_labels_valence.extend(labels_valence.cpu().numpy())\n",
    "            train_labels_energy.extend(labels_energy.cpu().numpy())\n",
    "            train_labels_danceability.extend(labels_danceability.cpu().numpy())\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        train_accuracy_valence = metrics.accuracy_score(train_labels_valence, train_predictions_valence)\n",
    "        train_accuracy_energy = metrics.accuracy_score(train_labels_energy, train_predictions_energy)\n",
    "        train_accuracy_danceability = metrics.accuracy_score(train_labels_danceability, train_predictions_danceability)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions_valence = []\n",
    "        val_predictions_energy = []\n",
    "        val_predictions_danceability = []\n",
    "        val_labels_valence = []\n",
    "        val_labels_energy = []\n",
    "        val_labels_danceability = []\n",
    "        progress_bar = tqdm(val_dataloader, desc='Validation', leave=False)\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_valence = batch['labels_valence'].to(device)\n",
    "            labels_energy = batch['labels_energy'].to(device)\n",
    "            labels_danceability = batch['labels_danceability'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels_valence=labels_valence, labels_energy=labels_energy, labels_danceability=labels_danceability)\n",
    "                loss, logits_valence, logits_energy, logits_danceability = outputs\n",
    "                val_loss += loss.item()\n",
    "                val_predictions_valence.extend(logits_valence.argmax(dim=-1).cpu().numpy())\n",
    "                val_predictions_energy.extend(logits_energy.argmax(dim=-1).cpu().numpy())\n",
    "                val_predictions_danceability.extend(logits_danceability.argmax(dim=-1).cpu().numpy())\n",
    "                val_labels_valence.extend(labels_valence.cpu().numpy())\n",
    "                val_labels_energy.extend(labels_energy.cpu().numpy())\n",
    "                val_labels_danceability.extend(labels_danceability.cpu().numpy())\n",
    "                progress_bar.set_postfix({'val_loss': loss.item()})\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_accuracy_valence = metrics.accuracy_score(val_labels_valence, val_predictions_valence)\n",
    "        val_accuracy_energy = metrics.accuracy_score(val_labels_energy, val_predictions_energy)\n",
    "        val_accuracy_danceability = metrics.accuracy_score(val_labels_danceability, val_predictions_danceability)\n",
    "\n",
    "        val_accuracy = (val_accuracy_valence + val_accuracy_energy + val_accuracy_danceability) / 3\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model2.pt')\n",
    "            \n",
    "    model.load_state_dict(torch.load('best_model2.pt'))\n",
    "    print(f'No. of Epoch: {epoch}; Validation accuracy: {val_accuracy * 100:.2f}%; train accuracies: valence {train_accuracy_valence * 100:.2f}%, energy {train_accuracy_energy * 100:.2f}%, danceability {train_accuracy_danceability * 100:.2f}%')\n",
    "    return val_accuracy, avg_val_loss, train_accuracy_valence, train_accuracy_energy, train_accuracy_danceability, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    params = {\n",
    "        'lr': trial.suggest_float('lr', 2e-5, 5e-5, log=True),\n",
    "        'num_epochs': trial.suggest_int('num_epochs', 4, 10),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 0.01, 0.3),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.05, 0.3),\n",
    "    }\n",
    "    val_accuracy, avg_val_loss, train_accuracy_valence, train_accuracy_energy, train_accuracy_danceability, train_loss = train_and_evaluate(params)\n",
    "    trial.set_user_attr(\"val_accuracy\", val_accuracy)\n",
    "    trial.set_user_attr(\"avg_val_loss\", avg_val_loss)\n",
    "    trial.set_user_attr(\"train_accuracy_valence\", train_accuracy_valence)\n",
    "    trial.set_user_attr(\"train_accuracy_energy\", train_accuracy_energy)\n",
    "    trial.set_user_attr(\"train_accuracy_danceability\", train_accuracy_danceability)\n",
    "    trial.set_user_attr(\"train_loss\", train_loss)\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-21 02:53:19,416] A new study created in memory with name: no-name-33576725-569d-490e-830f-dd56489858ac\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-05-21 02:54:49,059] Trial 0 failed with parameters: {'lr': 3.004027098177885e-05, 'num_epochs': 6, 'batch_size': 64, 'l2_regularization': 0.07661651345787301, 'dropout_rate': 0.09132757403062867} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 196, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_29292\\701095809.py\", line 9, in objective\n",
      "    val_accuracy, avg_val_loss, train_accuracy_valence, train_accuracy_energy, train_accuracy_danceability, train_loss = modified_train_and_evaluate(params)\n",
      "                                                                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Alex\\AppData\\Local\\Temp\\ipykernel_29292\\4064620243.py\", line 57, in modified_train_and_evaluate\n",
      "    train_loss += loss.item()\n",
      "                  ^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[W 2024-05-21 02:54:49,060] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[145], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m logging\u001b[38;5;241m.\u001b[39mset_verbosity_error()\n\u001b[0;32m      2\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest hyperparameters: \u001b[39m\u001b[38;5;124m'\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_params)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest value (negative validation loss): \u001b[39m\u001b[38;5;124m'\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_value)\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\optuna\\study\\_optimize.py:62\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 62\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     63\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\optuna\\study\\_optimize.py:159\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 159\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    161\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    163\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\optuna\\study\\_optimize.py:247\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    243\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    246\u001b[0m ):\n\u001b[1;32m--> 247\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\optuna\\study\\_optimize.py:196\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    199\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[144], line 9\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mobjective\u001b[39m(trial):\n\u001b[0;32m      2\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m2e-5\u001b[39m, \u001b[38;5;241m5e-5\u001b[39m, log\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m10\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;241m0.3\u001b[39m),\n\u001b[0;32m      8\u001b[0m     }\n\u001b[1;32m----> 9\u001b[0m     val_accuracy, avg_val_loss, train_accuracy_valence, train_accuracy_energy, train_accuracy_danceability, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodified_train_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     trial\u001b[38;5;241m.\u001b[39mset_user_attr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m, val_accuracy)\n\u001b[0;32m     11\u001b[0m     trial\u001b[38;5;241m.\u001b[39mset_user_attr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg_val_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, avg_val_loss)\n",
      "Cell \u001b[1;32mIn[140], line 57\u001b[0m, in \u001b[0;36mmodified_train_and_evaluate\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     54\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     55\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 57\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m train_predictions_valence\u001b[38;5;241m.\u001b[39mextend(logits_valence\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     59\u001b[0m train_predictions_energy\u001b[38;5;241m.\u001b[39mextend(logits_energy\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print('Best hyperparameters: ', study.best_params)\n",
    "print('Best value (negative validation loss): ', study.best_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Owen original BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 128\n",
    "class BertLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        self.bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        bert_output = self.bert_model([input_ids, attention_mask])\n",
    "        cls_token = bert_output.last_hidden_state[:, 0, :]\n",
    "        return cls_token\n",
    "\n",
    "def build_bert_model():\n",
    "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    bert_layer = BertLayer()\n",
    "    cls_token = bert_layer([input_ids, attention_mask])\n",
    "    cls_token = Dropout(0.3)(cls_token)\n",
    "\n",
    "    dense_valence = Dense(y_valence.shape[1], activation='softmax', name='valence_output', kernel_regularizer=tf.keras.regularizers.l2(0.01))(cls_token)\n",
    "    dense_energy = Dense(y_energy.shape[1], activation='softmax', name='energy_output', kernel_regularizer=tf.keras.regularizers.l2(0.01))(cls_token)\n",
    "    dense_danceability = Dense(y_danceability.shape[1], activation='softmax', name='danceability_output', kernel_regularizer=tf.keras.regularizers.l2(0.01))(cls_token)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=[dense_valence, dense_energy, dense_danceability])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_ids           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_mask      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_layer_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BertLayer</span>)         │                   │            │ attention_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ bert_layer_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ valence_output      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,307</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ energy_output       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,307</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ danceability_output │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,307</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_ids           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ attention_mask      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ bert_layer_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ input_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│ (\u001b[38;5;33mBertLayer\u001b[0m)         │                   │            │ attention_mask[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ bert_layer_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ valence_output      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │      \u001b[38;5;34m2,307\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ energy_output       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │      \u001b[38;5;34m2,307\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ danceability_output │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)         │      \u001b[38;5;34m2,307\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,921</span> (27.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,921\u001b[0m (27.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,921</span> (27.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,921\u001b[0m (27.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = build_bert_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 561ms/step - danceability_output_accuracy: 0.3695 - energy_output_accuracy: 0.3343 - loss: 3.7649 - valence_output_accuracy: 0.3540 - val_danceability_output_accuracy: 0.4167 - val_energy_output_accuracy: 0.3542 - val_loss: 3.5277 - val_valence_output_accuracy: 0.3958\n",
      "Epoch 2/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 523ms/step - danceability_output_accuracy: 0.3970 - energy_output_accuracy: 0.3431 - loss: 3.7302 - valence_output_accuracy: 0.3741 - val_danceability_output_accuracy: 0.4167 - val_energy_output_accuracy: 0.3333 - val_loss: 3.5124 - val_valence_output_accuracy: 0.3958\n",
      "Epoch 3/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 531ms/step - danceability_output_accuracy: 0.3628 - energy_output_accuracy: 0.4341 - loss: 3.5981 - valence_output_accuracy: 0.4067 - val_danceability_output_accuracy: 0.4271 - val_energy_output_accuracy: 0.3333 - val_loss: 3.5001 - val_valence_output_accuracy: 0.3958\n",
      "Epoch 4/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 534ms/step - danceability_output_accuracy: 0.4427 - energy_output_accuracy: 0.3412 - loss: 3.7449 - valence_output_accuracy: 0.3441 - val_danceability_output_accuracy: 0.4167 - val_energy_output_accuracy: 0.3333 - val_loss: 3.4951 - val_valence_output_accuracy: 0.3958\n",
      "Epoch 5/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 539ms/step - danceability_output_accuracy: 0.3637 - energy_output_accuracy: 0.4087 - loss: 3.7018 - valence_output_accuracy: 0.3505 - val_danceability_output_accuracy: 0.4167 - val_energy_output_accuracy: 0.3333 - val_loss: 3.4930 - val_valence_output_accuracy: 0.3958\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# 设置学习率调度器\n",
    "num_train_steps = len(X_train_input_ids) // 16 * 5  # 数据量 / batch_size * epochs\n",
    "num_warmup_steps = num_train_steps // 10  # 通常设置为训练步骤的10%\n",
    "\n",
    "optimizer = Adam(learning_rate=tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=2e-5,\n",
    "    decay_steps=num_train_steps,\n",
    "    end_learning_rate=0.0\n",
    "))\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss={'valence_output': 'categorical_crossentropy', \n",
    "                    'energy_output': 'categorical_crossentropy', \n",
    "                    'danceability_output': 'categorical_crossentropy'},\n",
    "              metrics={'valence_output': 'accuracy', \n",
    "                       'energy_output': 'accuracy', \n",
    "                       'danceability_output': 'accuracy'})\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_input_ids, X_train_attention_masks],\n",
    "    {'valence_output': y_train_valence, 'energy_output': y_train_energy, 'danceability_output': y_train_danceability},\n",
    "    validation_data=([X_val_input_ids, X_val_attention_masks], {'valence_output': y_val_valence, 'energy_output': y_val_energy, 'danceability_output': y_val_danceability}),\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, valence_output_loss, energy_output_loss, danceability_output_loss, accuracy_valence, accuracy_energy, accuracy_danceability = model.evaluate(\n",
    "    [X_test_input_ids, X_test_attention_masks], \n",
    "    [y_test_valence, y_test_energy, y_test_danceability]\n",
    ")\n",
    "\n",
    "print(f'Test Loss: {loss}, valence_output_loss: {valence_output_loss}, energy_output_loss: {energy_output_loss}, danceability_output_loss: {danceability_output_loss}')\n",
    "print(f'Test Accuracy Valence: {accuracy_valence}, Test Accuracy Energy: {accuracy_energy}, Test Accuracy Danceability: {accuracy_danceability}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have implemented lr and weight decay onto this, therefore it has warm up and decay now with l2 reg, and i only ran 5 epochs because its quite slow on my mac, I noticed the loss is still decreasing drastically, therefore i believe runing more epochs will eventually boost the acc by a lot, can you guys make it 10-15 epochs and test out whats going on at that. Thx, ill now push this version onto github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Alex\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "max_len = 128\n",
    "\n",
    "X_train_input_ids, X_train_attention_masks = tokenize(X_train, tokenizer, max_len)\n",
    "X_val_input_ids, X_val_attention_masks = tokenize(X_val, tokenizer, max_len)\n",
    "X_test_input_ids, X_test_attention_masks = tokenize(X_test, tokenizer, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoBERTaLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(RoBERTaLayer, self).__init__(**kwargs)\n",
    "        self.roberta_model = TFRobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_ids, attention_mask = inputs\n",
    "        roberta_output = self.roberta_model([input_ids, attention_mask])\n",
    "        cls_token = roberta_output.last_hidden_state[:, 0, :]\n",
    "        return cls_token\n",
    "\n",
    "def build_bert_model():\n",
    "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    bert_layer = BertLayer()\n",
    "    cls_token = bert_layer([input_ids, attention_mask])\n",
    "    cls_token = Dropout(0.3)(cls_token)\n",
    "\n",
    "    dense_valence = Dense(y_valence.shape[1], activation='softmax', name='valence_output', kernel_regularizer=tf.keras.regularizers.l2(0.01))(cls_token)\n",
    "    dense_energy = Dense(y_energy.shape[1], activation='softmax', name='energy_output', kernel_regularizer=tf.keras.regularizers.l2(0.01))(cls_token)\n",
    "    dense_danceability = Dense(y_danceability.shape[1], activation='softmax', name='danceability_output', kernel_regularizer=tf.keras.regularizers.l2(0.01))(cls_token)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=[dense_valence, dense_energy, dense_danceability])\n",
    "    return model\n",
    "\n",
    "def build_roberta_model():\n",
    "    input_ids = Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n",
    "    attention_mask = Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n",
    "    \n",
    "    roberta_layer = RoBERTaLayer()\n",
    "    cls_token = roberta_layer([input_ids, attention_mask])\n",
    "    cls_token = Dropout(0.3)(cls_token)\n",
    "\n",
    "    dense_valence = Dense(y_valence.shape[1], activation='softmax', name='valence_output', kernel_regularizer=tf.keras.regularizers.l2(0.01))(cls_token)\n",
    "    dense_energy = Dense(y_energy.shape[1], activation='softmax', name='energy_output', kernel_regularizer=tf.keras.regularizers.l2(0.01))(cls_token)\n",
    "    dense_danceability = Dense(y_danceability.shape[1], activation='softmax', name='danceability_output', kernel_regularizer=tf.keras.regularizers.l2(0.01))(cls_token)\n",
    "    \n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=[dense_valence, dense_energy, dense_danceability])\n",
    "    return model\n",
    "\n",
    "def get_optimizer_and_scheduler(num_train_steps, num_warmup_steps):\n",
    "    optimizer = Adam(learning_rate=2e-5, weight_decay=0.01)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "    return optimizer, lr_scheduler\n",
    "\n",
    "num_train_steps = len(X_train_input_ids) // 16 * 5\n",
    "num_warmup_steps = num_train_steps // 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_roberta_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 586ms/step - danceability_output_accuracy: 0.3918 - energy_output_accuracy: 0.4023 - loss: 3.7120 - valence_output_accuracy: 0.3704 - val_danceability_output_accuracy: 0.2917 - val_energy_output_accuracy: 0.3438 - val_loss: 3.7568 - val_valence_output_accuracy: 0.3958\n",
      "Epoch 2/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 551ms/step - danceability_output_accuracy: 0.3769 - energy_output_accuracy: 0.3743 - loss: 3.7413 - valence_output_accuracy: 0.4184 - val_danceability_output_accuracy: 0.2917 - val_energy_output_accuracy: 0.3438 - val_loss: 3.7448 - val_valence_output_accuracy: 0.3958\n",
      "Epoch 3/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 529ms/step - danceability_output_accuracy: 0.4002 - energy_output_accuracy: 0.4204 - loss: 3.6848 - valence_output_accuracy: 0.3888 - val_danceability_output_accuracy: 0.2917 - val_energy_output_accuracy: 0.3438 - val_loss: 3.7368 - val_valence_output_accuracy: 0.3958\n",
      "Epoch 4/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 528ms/step - danceability_output_accuracy: 0.4027 - energy_output_accuracy: 0.3626 - loss: 3.7353 - valence_output_accuracy: 0.3508 - val_danceability_output_accuracy: 0.2917 - val_energy_output_accuracy: 0.3438 - val_loss: 3.7319 - val_valence_output_accuracy: 0.3958\n",
      "Epoch 5/5\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 535ms/step - danceability_output_accuracy: 0.4068 - energy_output_accuracy: 0.3715 - loss: 3.7035 - valence_output_accuracy: 0.3584 - val_danceability_output_accuracy: 0.2917 - val_energy_output_accuracy: 0.3438 - val_loss: 3.7303 - val_valence_output_accuracy: 0.3958\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 810ms/step - danceability_output_accuracy: 0.4590 - energy_output_accuracy: 0.3313 - loss: 3.5928 - valence_output_accuracy: 0.4023\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 7, got 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[154], line 30\u001b[0m\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer,\n\u001b[0;32m     14\u001b[0m               loss\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalence_output\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     15\u001b[0m                     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_output\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_output\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     19\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdanceability_output\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     21\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     22\u001b[0m     [X_train_input_ids, X_train_attention_masks],\n\u001b[0;32m     23\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalence_output\u001b[39m\u001b[38;5;124m'\u001b[39m: y_train_valence, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124menergy_output\u001b[39m\u001b[38;5;124m'\u001b[39m: y_train_energy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdanceability_output\u001b[39m\u001b[38;5;124m'\u001b[39m: y_train_danceability},\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping]\n\u001b[0;32m     28\u001b[0m )\n\u001b[1;32m---> 30\u001b[0m roberta_loss, roberta_valence_output_loss, roberta_energy_output_loss, roberta_danceability_output_loss, roberta_accuracy_valence, roberta_accuracy_energy, roberta_accuracy_danceability \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m     31\u001b[0m     [X_test_input_ids, X_test_attention_masks], \n\u001b[0;32m     32\u001b[0m     [y_test_valence, y_test_energy, y_test_danceability]\n\u001b[0;32m     33\u001b[0m )\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroberta_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, valence_output_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroberta_valence_output_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, energy_output_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroberta_energy_output_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, danceability_output_loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroberta_accuracy_valence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Accuracy Valence: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroberta_accuracy_valence\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy Energy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroberta_accuracy_energy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test Accuracy Danceability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroberta_accuracy_danceability\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 7, got 4)"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# 设置学习率调度器\n",
    "num_train_steps = len(X_train_input_ids) // 16 * 5\n",
    "num_warmup_steps = num_train_steps // 10 \n",
    "\n",
    "optimizer = Adam(learning_rate=tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=2e-5,\n",
    "    decay_steps=num_train_steps,\n",
    "    end_learning_rate=0.0\n",
    "))\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss={'valence_output': 'categorical_crossentropy', \n",
    "                    'energy_output': 'categorical_crossentropy', \n",
    "                    'danceability_output': 'categorical_crossentropy'},\n",
    "              metrics={'valence_output': 'accuracy', \n",
    "                       'energy_output': 'accuracy', \n",
    "                       'danceability_output': 'accuracy'})\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_input_ids, X_train_attention_masks],\n",
    "    {'valence_output': y_train_valence, 'energy_output': y_train_energy, 'danceability_output': y_train_danceability},\n",
    "    validation_data=([X_val_input_ids, X_val_attention_masks], {'valence_output': y_val_valence, 'energy_output': y_val_energy, 'danceability_output': y_val_danceability}),\n",
    "    epochs=5,\n",
    "    batch_size=16,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_loss, roberta_valence_output_loss, roberta_energy_output_loss, roberta_danceability_output_loss, roberta_accuracy_valence, roberta_accuracy_energy, roberta_accuracy_danceability = model.evaluate(\n",
    "    [X_test_input_ids, X_test_attention_masks], \n",
    "    [y_test_valence, y_test_energy, y_test_danceability]\n",
    ")\n",
    "\n",
    "\n",
    "print(f'Test Loss: {roberta_loss}, valence_output_loss: {roberta_valence_output_loss}, energy_output_loss: {roberta_energy_output_loss}, danceability_output_loss: {roberta_accuracy_valence}')\n",
    "print(f'Test Accuracy Valence: {roberta_accuracy_valence}, Test Accuracy Energy: {roberta_accuracy_energy}, Test Accuracy Danceability: {roberta_accuracy_danceability}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelRoBERTa(torch.nn.Module):\n",
    "    def __init__(self, model_name, num_labels_valence, num_labels_energy, num_labels_danceability, dropout_rate):\n",
    "        super(MultiLabelRoBERTa, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(model_name)\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.classifier_valence = torch.nn.Linear(self.roberta.config.hidden_size, num_labels_valence)\n",
    "        self.classifier_energy = torch.nn.Linear(self.roberta.config.hidden_size, num_labels_energy)\n",
    "        self.classifier_danceability = torch.nn.Linear(self.roberta.config.hidden_size, num_labels_danceability)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, labels_valence=None, labels_energy=None, labels_danceability=None):\n",
    "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_token = outputs[0][:, 0, :]\n",
    "        cls_token = self.dropout(cls_token)\n",
    "        \n",
    "        logits_valence = self.classifier_valence(cls_token)\n",
    "        logits_energy = self.classifier_energy(cls_token)\n",
    "        logits_danceability = self.classifier_danceability(cls_token)\n",
    "        \n",
    "        loss = 0\n",
    "        if labels_valence is not None and labels_energy is not None and labels_danceability is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss_valence = loss_fct(logits_valence, labels_valence)\n",
    "            loss_energy = loss_fct(logits_energy, labels_energy)\n",
    "            loss_danceability = loss_fct(logits_danceability, labels_danceability)\n",
    "            loss = (loss_valence + loss_energy + loss_danceability) / 3\n",
    "        \n",
    "        return loss, logits_valence, logits_energy, logits_danceability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_roberta(params):\n",
    "    lr = params['lr']\n",
    "    num_epochs = int(params['num_epochs'])\n",
    "    batch_size = int(params['batch_size'])\n",
    "    weight_decay = params['weight_decay']\n",
    "    dropout_rate = params['dropout_rate']\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = MultiLabelRoBERTa('roberta-base', num_labels_valence=3, num_labels_energy=3, num_labels_danceability=3, dropout_rate=dropout_rate).to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    train_dataset = MultiLabelDataset(X_train_input_ids, X_train_attention_masks, y_train_valence, y_train_energy, y_train_danceability)\n",
    "    val_dataset = MultiLabelDataset(X_val_input_ids, X_val_attention_masks, y_val_valence, y_val_energy, y_val_danceability)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    num_training_steps = len(train_dataloader) * num_epochs\n",
    "    warmup_steps = int(0.1 * num_training_steps)\n",
    "    \n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=warmup_steps,\n",
    "        num_training_steps=num_training_steps)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_accuracy = float(0)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_predictions_valence = []\n",
    "        train_predictions_energy = []\n",
    "        train_predictions_danceability = []\n",
    "        train_labels_valence = []\n",
    "        train_labels_energy = []\n",
    "        train_labels_danceability = []\n",
    "        progress_bar = tqdm(train_dataloader, desc='Training', leave=False)\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_valence = batch['labels_valence'].to(device)\n",
    "            labels_energy = batch['labels_energy'].to(device)\n",
    "            labels_danceability = batch['labels_danceability'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels_valence=labels_valence, labels_energy=labels_energy, labels_danceability=labels_danceability)\n",
    "            loss, logits_valence, logits_energy, logits_danceability = outputs\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            train_predictions_valence.extend(logits_valence.argmax(dim=-1).cpu().numpy())\n",
    "            train_predictions_energy.extend(logits_energy.argmax(dim=-1).cpu().numpy())\n",
    "            train_predictions_danceability.extend(logits_danceability.argmax(dim=-1).cpu().numpy())\n",
    "            train_labels_valence.extend(labels_valence.cpu().numpy())\n",
    "            train_labels_energy.extend(labels_energy.cpu().numpy())\n",
    "            train_labels_danceability.extend(labels_danceability.cpu().numpy())\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        train_accuracy_valence = metrics.accuracy_score(train_labels_valence, train_predictions_valence)\n",
    "        train_accuracy_energy = metrics.accuracy_score(train_labels_energy, train_predictions_energy)\n",
    "        train_accuracy_danceability = metrics.accuracy_score(train_labels_danceability, train_predictions_danceability)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_predictions_valence = []\n",
    "        val_predictions_energy = []\n",
    "        val_predictions_danceability = []\n",
    "        val_labels_valence = []\n",
    "        val_labels_energy = []\n",
    "        val_labels_danceability = []\n",
    "        progress_bar = tqdm(val_dataloader, desc='Validation', leave=False)\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels_valence = batch['labels_valence'].to(device)\n",
    "            labels_energy = batch['labels_energy'].to(device)\n",
    "            labels_danceability = batch['labels_danceability'].to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, attention_mask=attention_mask, labels_valence=labels_valence, labels_energy=labels_energy, labels_danceability=labels_danceability)\n",
    "                loss, logits_valence, logits_energy, logits_danceability = outputs\n",
    "                val_loss += loss.item()\n",
    "                val_predictions_valence.extend(logits_valence.argmax(dim=-1).cpu().numpy())\n",
    "                val_predictions_energy.extend(logits_energy.argmax(dim=-1).cpu().numpy())\n",
    "                val_predictions_danceability.extend(logits_danceability.argmax(dim=-1).cpu().numpy())\n",
    "                val_labels_valence.extend(labels_valence.cpu().numpy())\n",
    "                val_labels_energy.extend(labels_energy.cpu().numpy())\n",
    "                val_labels_danceability.extend(labels_danceability.cpu().numpy())\n",
    "                progress_bar.set_postfix({'val_loss': loss.item()})\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_accuracy_valence = metrics.accuracy_score(val_labels_valence, val_predictions_valence)\n",
    "        val_accuracy_energy = metrics.accuracy_score(val_labels_energy, val_predictions_energy)\n",
    "        val_accuracy_danceability = metrics.accuracy_score(val_labels_danceability, val_predictions_danceability)\n",
    "\n",
    "        val_accuracy = (val_accuracy_valence + val_accuracy_energy + val_accuracy_danceability) / 3\n",
    "\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), 'best_model2.pt')\n",
    "            \n",
    "    model.load_state_dict(torch.load('best_model2.pt'))\n",
    "    print(f'No. of Epoch: {epoch}; Validation accuracy: {val_accuracy * 100:.2f}%; train accuracies: valence {train_accuracy_valence * 100:.2f}%, energy {train_accuracy_energy * 100:.2f}%, danceability {train_accuracy_danceability * 100:.2f}%')\n",
    "    return val_accuracy, avg_val_loss, train_accuracy_valence, train_accuracy_energy, train_accuracy_danceability, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_roberta(trial):\n",
    "    params = {\n",
    "        'lr': trial.suggest_float('lr', 2e-5, 5e-5, log=True),\n",
    "        'num_epochs': trial.suggest_int('num_epochs', 4, 10),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64]),\n",
    "        'weight_decay': trial.suggest_float('weight_decay', 0.01, 0.3),\n",
    "        'dropout_rate': trial.suggest_float('dropout_rate', 0.05, 0.3),\n",
    "    }\n",
    "    val_accuracy, avg_val_loss, train_accuracy_valence, train_accuracy_energy, train_accuracy_danceability, train_loss = train_and_evaluate_roberta(params)\n",
    "    trial.set_user_attr(\"val_accuracy\", val_accuracy)\n",
    "    trial.set_user_attr(\"avg_val_loss\", avg_val_loss)\n",
    "    trial.set_user_attr(\"train_accuracy_valence\", train_accuracy_valence)\n",
    "    trial.set_user_attr(\"train_accuracy_energy\", train_accuracy_energy)\n",
    "    trial.set_user_attr(\"train_accuracy_danceability\", train_accuracy_danceability)\n",
    "    trial.set_user_attr(\"train_loss\", train_loss)\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-21 03:30:02,593] A new study created in memory with name: no-name-acfb86c7-63a1-4e71-a741-752f7f1351f6\n",
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[I 2024-05-21 03:31:10,816] Trial 0 finished with value: 1.2109988331794739 and parameters: {'lr': 4.384470994489393e-05, 'num_epochs': 10, 'batch_size': 16, 'weight_decay': 0.2860366644807206, 'dropout_rate': 0.23459909429263764}. Best is trial 0 with value: 1.2109988331794739.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Epoch: 9; Validation accuracy: 50.69%; train accuracies: valence 91.15%, energy 96.61%, danceability 96.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[I 2024-05-21 03:31:53,847] Trial 1 finished with value: 1.0424423615137737 and parameters: {'lr': 2.0285129598819828e-05, 'num_epochs': 7, 'batch_size': 32, 'weight_decay': 0.2840274803138773, 'dropout_rate': 0.09074442945309126}. Best is trial 1 with value: 1.0424423615137737.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Epoch: 6; Validation accuracy: 51.39%; train accuracies: valence 56.77%, energy 59.11%, danceability 61.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[I 2024-05-21 03:32:58,306] Trial 2 finished with value: 1.3006866077582042 and parameters: {'lr': 3.536790789689419e-05, 'num_epochs': 10, 'batch_size': 16, 'weight_decay': 0.08689906860285412, 'dropout_rate': 0.2237163346888606}. Best is trial 1 with value: 1.0424423615137737.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Epoch: 9; Validation accuracy: 54.51%; train accuracies: valence 95.83%, energy 96.35%, danceability 97.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[I 2024-05-21 03:33:52,944] Trial 3 finished with value: 1.0459251801172893 and parameters: {'lr': 3.610123408214526e-05, 'num_epochs': 9, 'batch_size': 32, 'weight_decay': 0.18054326543648724, 'dropout_rate': 0.1693009320676429}. Best is trial 1 with value: 1.0424423615137737.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Epoch: 8; Validation accuracy: 50.69%; train accuracies: valence 78.65%, energy 79.43%, danceability 84.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[I 2024-05-21 03:34:44,043] Trial 4 finished with value: 1.2035074432690938 and parameters: {'lr': 4.2946125666529956e-05, 'num_epochs': 8, 'batch_size': 16, 'weight_decay': 0.031957494834824955, 'dropout_rate': 0.23271782673791308}. Best is trial 1 with value: 1.0424423615137737.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Epoch: 7; Validation accuracy: 48.61%; train accuracies: valence 84.11%, energy 88.28%, danceability 90.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[I 2024-05-21 03:35:32,446] Trial 5 finished with value: 1.0352118611335754 and parameters: {'lr': 2.5536352909221795e-05, 'num_epochs': 8, 'batch_size': 32, 'weight_decay': 0.06635864020785948, 'dropout_rate': 0.12413734556795548}. Best is trial 5 with value: 1.0352118611335754.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Epoch: 7; Validation accuracy: 49.65%; train accuracies: valence 64.84%, energy 65.89%, danceability 75.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[I 2024-05-21 03:36:29,448] Trial 6 finished with value: 1.0600566864013672 and parameters: {'lr': 2.0316497489308853e-05, 'num_epochs': 9, 'batch_size': 16, 'weight_decay': 0.26850133740329657, 'dropout_rate': 0.22690355822248143}. Best is trial 5 with value: 1.0352118611335754.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Epoch: 8; Validation accuracy: 52.08%; train accuracies: valence 79.17%, energy 84.38%, danceability 82.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[I 2024-05-21 03:50:48,609] Trial 7 finished with value: 1.0242444276809692 and parameters: {'lr': 2.832711467215106e-05, 'num_epochs': 9, 'batch_size': 64, 'weight_decay': 0.17436334634435374, 'dropout_rate': 0.059400665179551546}. Best is trial 7 with value: 1.0242444276809692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Epoch: 8; Validation accuracy: 45.49%; train accuracies: valence 53.91%, energy 57.03%, danceability 68.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[I 2024-05-21 03:51:44,111] Trial 8 finished with value: 1.3093683520952861 and parameters: {'lr': 3.368508129655881e-05, 'num_epochs': 10, 'batch_size': 16, 'weight_decay': 0.03044307001614459, 'dropout_rate': 0.12874849588960435}. Best is trial 7 with value: 1.0242444276809692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Epoch: 9; Validation accuracy: 51.39%; train accuracies: valence 95.83%, energy 97.66%, danceability 97.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Alex\\anaconda3\\envs\\pt\\Lib\\site-packages\\transformers\\optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "[I 2024-05-21 03:52:31,647] Trial 9 finished with value: 1.0784907341003418 and parameters: {'lr': 4.3168364339364895e-05, 'num_epochs': 9, 'batch_size': 32, 'weight_decay': 0.2648664296201911, 'dropout_rate': 0.2877628419608818}. Best is trial 7 with value: 1.0242444276809692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Epoch: 8; Validation accuracy: 49.65%; train accuracies: valence 75.00%, energy 72.14%, danceability 76.82%\n",
      "Best hyperparameters:  {'lr': 2.832711467215106e-05, 'num_epochs': 9, 'batch_size': 64, 'weight_decay': 0.17436334634435374, 'dropout_rate': 0.059400665179551546}\n",
      "Best value (negative validation loss):  1.0242444276809692\n"
     ]
    }
   ],
   "source": [
    "logging.set_verbosity_error()\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective_roberta, n_trials=10)\n",
    "\n",
    "print('Best hyperparameters: ', study.best_params)\n",
    "print('Best value (negative validation loss): ', study.best_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
